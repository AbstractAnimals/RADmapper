import sys
import utils
from IO_interface import samIterator
from utils.GenomeLoader import GenomeLoader
from utils import DNA_tools, utils_logging
import os
import logging, threading
from optparse import OptionParser
from overlap import merge_ranges
import random
from collections import OrderedDict

class FuncThread(threading.Thread):
    def __init__(self, target, *args):
        self._target = target
        self._args = args
        threading.Thread.__init__(self)
        self.returned_value=None
 
    def run(self):
        self.returned_value = self._target(*self._args)
        
    def get_returned_value(self):
        return self.returned_value
 

def bam_to_fastq(bam_file,genome_file):
    stream = utils.get_sam_stream(bam_file)
    genome_loader=GenomeLoader(genome_file, keep_in_memory=True, keep_until_done=True)
    for line in stream:
        sam_record = samIterator.Sam_record(line)
        sam_record = fix_sam_record_from_reference(sam_record, genome_loader)
        print sam_record_to_fastq(sam_record)

def bam_detect_RAD_sites(bam_file, genome_file, restriction_site_seq='TGCAG', extension=1):
    stream = utils.get_sam_stream(bam_file, options='-F 128')
    genome_loader=GenomeLoader(genome_file, keep_in_memory=True, keep_until_done=True)
    count_good_reads=0
    count_not_good_reads=0
    count_line=0
    all_sites_position=set()
    
    for line in stream:
        sam_record = samIterator.Sam_record(line)
        if sam_record.is_first_read() and not sam_record.is_unmapped():
            ref=sam_record.get_reference_name()
            pos=sam_record.get_position()
            align_length=sam_record.get_alignment_length()
            read = sam_record.get_query_sequence()
            (header,sequence) = genome_loader.get_chr(ref)
            if sam_record.is_reverse_strand():
                res_site_on_genome = DNA_tools.rev_complements(sequence[pos+align_length-len(restriction_site_seq)-1:pos+align_length-1])
                res_site_on_read = read[align_length-len(restriction_site_seq)-1:align_length-1]
                pos+=align_length-len(restriction_site_seq)+1
                
            else:
                res_site_on_genome=sequence[pos-1:pos+len(restriction_site_seq)-1]
                res_site_on_read = read[:len(restriction_site_seq)]    
            if res_site_on_genome==restriction_site_seq:
                count_good_reads+=1
                if sam_record.is_reverse_strand():
                    strand ='-'
                else:
                    strand ='+'
                all_sites_position.add((ref,pos,strand))
                #print ref, pos, strand
            else:
                count_not_good_reads+=1
        count_line+=1
        if count_line%100000==0:
            print count_line,len(all_sites_position),count_good_reads,count_not_good_reads
    print "%s good sites"%(len(all_sites_position))
    print "%s good read"%(count_good_reads)
    print "%s bad reads"%(count_not_good_reads)
    
    
def load_known_sites_old(know_sites_file):
    all_sites={}
    try:
        open_file = open(know_sites_file)
        for line in open_file:
            reference, position = line.strip().split()
            all_sites[(reference, int(position) )]=1
    finally:
        open_file.close()
    return all_sites

def load_known_sites(know_sites_file):
    all_sites={}
    open_file = open(know_sites_file)
    for line in open_file:
        sp_line = line.strip().split()
        if len(sp_line)==3:
            reference, position, strand =sp_line
            all_sites[(reference, int(position) , strand )]=1
        elif len(sp_line)==4:
            reference, position, strand , allele_seq = sp_line
            all_sites[(reference, int(position) , strand, allele_seq)]=1
    open_file.close()
    return all_sites
        
        
def load_from_sites_generator(known_sites, bam_file, restriction_site_seq, options=''):
    """This function return a generator that iterates over read pair where both pairs are mapping.
    @return a tuple containing read1 read2 and the restriction site position."""
    #stream = utils.get_sam_stream(bam_file, options='-f 2')
    stream = utils.get_sam_stream(bam_file, options=options)
    all_unmatched_read1={}
    all_unmatched_read2={}
    count_line=0
    for line in stream:
        count_line+=1
        #if count_line%10000==0:
        #    print count_line, len(all_unmatched_read1), len(all_unmatched_read2)
        sam_record = samIterator.Sam_record(line)
        if sam_record.is_first_read():
            sam_record_r2 = all_unmatched_read2.pop(sam_record.get_query_name(),None)
            info=None
            if not sam_record.is_unmapped():
                info = get_RAD_site_from_sam_record(sam_record, known_sites, restriction_site_seq)
            if sam_record_r2:
                if info:
                    yield((sam_record,sam_record_r2, info))
            else:
                all_unmatched_read1[sam_record.get_query_name()]=sam_record
        else:
            sam_record_r1 = all_unmatched_read1.pop(sam_record.get_query_name(),None)
            if sam_record_r1:
                info = get_RAD_site_from_sam_record(sam_record_r1, known_sites, restriction_site_seq)
                if info:
                    yield((sam_record_r1, sam_record, info))
            else:
                all_unmatched_read2[sam_record.get_query_name()]=sam_record

def load_sorted_readend_from_sites_generator(known_sites, bam_file, restriction_site_seq):
    """This function return a generator that iterates over read pair where both pairs are mapping the pairs are sorted by read1 and then read2 location.
    @return a tuple containing read1 read2 and the restriction site position."""
    current_read1_location=None
    sam_pair_generator = load_from_sites_generator(known_sites, bam_file, restriction_site_seq)
    all_reads_stored=[]
    for sam_pair in sam_pair_generator:
        sam_record_r1, sam_record_r2, site_info =sam_pair
        if current_read1_location is None:
            current_read1_location=int(sam_record_r1.get_position())
            all_reads_stored=[sam_pair]
        elif current_read1_location==int(sam_record_r1.get_position()):
            all_reads_stored.append(sam_pair)
        else:
            all_reads_stored.sort(cmp=compare_read2)
            for pair in all_reads_stored:
                yield pair
            current_read1_location=int(sam_record_r1.get_position())
            all_reads_stored=[sam_pair]

def compare_read2(sam_pair1, sam_pair2):
    sam_record_r1, sam_record_r2_1, site_info =sam_pair1
    sam_record_r1, sam_record_r2_2, site_info =sam_pair2
    return int(sam_record_r2_1.get_position()) - int(sam_record_r2_2.get_position())
    
    
def load_duplicates_from_sites_generator(known_sites, bam_file, restriction_site_seq):
    """This function return a generator that iterates over list of read pairs where both pairs are mapping.
    @return a list of tuples where all are duplicates of one another. Each tuple contains read1 read2 and the restriction site position."""
    sam_pair_generator = load_sorted_readend_from_sites_generator(known_sites, bam_file, restriction_site_seq)
    next_chunck=[]
    first_of_chunck=None
    for sam_record_r1,sam_record_r2, site_info in sam_pair_generator:
        if first_of_chunck is None:
            first_of_chunck=(sam_record_r1,sam_record_r2, site_info)
            next_chunck.append(first_of_chunck)
        elif is_comparable_for_duplicate(first_of_chunck, sam_record_r1, sam_record_r2):
            next_chunck.append((sam_record_r1, sam_record_r2, site_info))
        else:
            if len(next_chunck)>0:
                yield next_chunck
            first_of_chunck=(sam_record_r1,sam_record_r2, site_info)
            next_chunck=[first_of_chunck]
    if len(next_chunck)>0:
        yield next_chunck
            
def is_comparable_for_duplicate(first_of_chunck, sam_record_r1, sam_record_r2):
    foc_r1, foc_r2, foc_site_info = first_of_chunck
    #print "%s == %s =%s"%(foc_r1.get_reference_name(),sam_record_r1.get_reference_name(), foc_r1.get_reference_name() == sam_record_r1.get_reference_name())
    #print "%s == %s =%s"%(foc_r1.get_position(), sam_record_r1.get_position(), foc_r1.get_position() == sam_record_r1.get_position())
    #print "%s == %s =%s"%(foc_r1.is_reverse_strand(), sam_record_r1.is_reverse_strand(), foc_r1.is_reverse_strand() == sam_record_r1.is_reverse_strand())
    #print "%s == %s =%s"%(foc_r2.get_reference_name(),sam_record_r2.get_reference_name(), foc_r2.get_reference_name() == sam_record_r2.get_reference_name())
    #print "%s == %s =%s"%(foc_r2.get_position(), sam_record_r2.get_position(), foc_r2.get_position() == sam_record_r2.get_position())
    #print "%s == %s =%s"%(foc_r2.is_reverse_strand(), sam_record_r2.is_reverse_strand(), foc_r2.is_reverse_strand() == sam_record_r2.is_reverse_strand())
    retval =  foc_r1.get_reference_name() == sam_record_r1.get_reference_name() and \
              foc_r1.get_position() == sam_record_r1.get_position() and \
              foc_r1.is_reverse_strand() == sam_record_r1.is_reverse_strand()
    if retval:
        retval =  foc_r2.get_reference_name() == sam_record_r2.get_reference_name() and \
                  foc_r2.get_position() == sam_record_r2.get_position() and \
                  foc_r2.is_reverse_strand() == sam_record_r2.is_reverse_strand()
    return retval

class File_factory():
    def __init__(self, max_file_open=1000):
        self.all_file_paths={}
        self.open_files=OrderedDict()
        self.in_dir_counter=0
        self.dir_counter=0
        self.max_file_open=max_file_open
        
        
    def get_filename_from_site(self, reference,position,strand, allele=None):
        if allele:
            file_path = self.all_file_paths.get('%s_%s_%s_%s'%(reference,position,strand,allele))
        else:
            file_path = self.all_file_paths.get('%s_%s_%s'%(reference,position,strand))
        if not file_path:
            self.in_dir_counter+=1
            if self.in_dir_counter>50 or self.dir_counter==0:
                self.in_dir_counter=1
                self.dir_counter+=1
                if not os.path.exists('%s_dir'%self.dir_counter):
                    os.mkdir('%s_dir'%self.dir_counter)
                    logging.debug("create %s_dir"%self.dir_counter)
            if allele:
                file_path='%s_dir/site_%s_%s_%s_%s.fastq'%(self.dir_counter,reference,position,strand,allele)
                self.all_file_paths['%s_%s_%s_%s'%(reference,position,strand,allele)]=file_path
            else:
                file_path='%s_dir/site_%s_%s_%s.fastq'%(self.dir_counter,reference,position,strand)
                self.all_file_paths['%s_%s_%s'%(reference,position,strand)]=file_path
            logging.debug("create %s"%file_path)
        return file_path
    
    def get_openfile_from_site(self, reference,position,strand, allele=None):
        file_path=self.get_filename_from_site(reference, position, strand, allele)
        open_file = self.open_files.get(file_path)
        if not open_file:
            if len(self.open_files)>self.max_file_open:
                (old_file_path,open_file) = self.open_files.popitem(last=False)
                logging.debug('Close %s'%(old_file_path))
                open_file.close()
            open_file=open(file_path,'a')
            self.open_files[file_path]=open_file
        return open_file
    
    def get_number_file_open(self):
        return len(self.open_files)
    
    def close(self):
        for open_file in self.open_files.values():
            open_file.close()
        self.list_file_open=[]
        
    def __del__(self):
        self.close()

def load_from_sites_and_add_snp_generator(known_sites_file, bam_file, snp_list_file, restriction_site_seq='TGCAG'):
    """This function return a generator that iterates read pairs where both pairs are mapping.
    @return a list of tuples where all are duplicates of one another. Each tuple contains read1 read2 and the restriction site position."""
    known_sites = load_known_sites(known_sites_file)
    #genome_loader=GenomeLoader(genome_file, keep_in_memory=True, keep_until_done=True)
    list_of_pair_generator = load_duplicates_from_sites_generator(known_sites, bam_file, restriction_site_seq)
    proba_duplicate=0.9
    list_snps=read_snp_file_per_sites(snp_list_file)
    for list_of_pair in list_of_pair_generator:
        sam_record_r1,sam_record_r2, site_info = list_of_pair[0]
        snp_list=list_snps.get(site_info)
        if snp_list:
            for snp_position,snp_base in snp_list:
                overlap_position_r1 = find_overlap_read_position(sam_record_r1, snp_position)
                overlap_position_r2 = find_overlap_read_position(sam_record_r2, snp_position)
                #check that the snp overlap with this read set
                if overlap_position_r1:
                    for sam_record_r1,sam_record_r2, site_info in list_of_pair:
                        if random.random()<proba_duplicate:
                            seq=sam_record_r1.get_query_sequence()
                            seq=seq[:overlap_position_r1]+snp_base+seq[overlap_position_r1+1:]
                            sam_record_r1.set_query_sequence(seq)
                            sam_record_r1.set_query_name(sam_record_r1.get_query_name()+'_SNP%s%s'%(overlap_position_r1,snp_base))
                if overlap_position_r2:
                    for sam_record_r1,sam_record_r2, site_info in list_of_pair:
                        if random.random()<proba_duplicate:
                            seq=sam_record_r2.get_query_sequence()
                            seq=seq[:overlap_position_r2]+snp_base+seq[overlap_position_r2+1:]
                            sam_record_r2.set_query_sequence(seq)
                            sam_record_r2.set_query_name(sam_record_r2.get_query_name()+'_SNP%s%s'%(overlap_position_r2,snp_base))
        for pair_of_reads in list_of_pair:
            yield pair_of_reads
        
def read_snp_file_per_sites(snp_list_file):
    all_snps_per_site={}
    open_snp_file = open(snp_list_file)
    for line in open_snp_file:
        sp_line=line.strip().split()
        site_info = (sp_line[0], int(sp_line[1]), sp_line[2])
        
        if not all_snps_per_site.has_key(site_info):
            all_snps_per_site[site_info]=[(int(sp_line[3]),sp_line[4])]
        else:
            all_snps_per_site.get(site_info).append((int(sp_line[3]),sp_line[4]))
    open_snp_file.close()
    return all_snps_per_site
    

def find_overlap_read_position(read, position):
    s=read.get_position() 
    if position<s:
        return None
    e=s+len(read.get_query_sequence())
    if position >= e:
        return None 
    return position-s


def process_bam_file(known_sites_file, bam_file, genome_file, snp_list_file=None, restriction_site_seq='TGCAG', file_factory=None):
    known_sites = load_known_sites(known_sites_file)
    #genome_loader=GenomeLoader(genome_file, keep_in_memory=True, keep_until_done=True)
    if snp_list_file:
        sam_pair_generator = load_from_sites_and_add_snp_generator(known_sites_file, bam_file, snp_list_file, restriction_site_seq)
    else:
        sam_pair_generator = load_from_sites_generator(known_sites, bam_file, restriction_site_seq)
    current_site=None
    current_open_file=None
    count_read=0
    all_sites_coverage={}
    close_file_factory=False
    if file_factory is None:
        file_factory=File_factory()
        close_file_factory=True
    site_to_list_of_range={}
    for sam_record_r1,sam_record_r2, site_info in sam_pair_generator:
        count_read+=1
        if count_read%10000==0:
            print count_read,file_factory.get_number_file_open()
        reference, position, strand = site_info
        if current_site!=site_info:
            current_open_file = file_factory.get_openfile_from_site(reference, position, strand)
            current_site=site_info
        
        if not all_sites_coverage.has_key(site_info):
            all_sites_coverage[site_info]=1
        else:
            all_sites_coverage[site_info]+=1
            
        rgid=sam_record_r1.get_tag('RG')
        #sam_record_r2 = fix_sam_record_from_reference(sam_record_r2, genome_loader)
        list_of_range = site_to_list_of_range.get(site_info)
        if not list_of_range:
            list_of_range=[]
            site_to_list_of_range[site_info]=list_of_range
        list_of_range.append((sam_record_r2.get_position(), sam_record_r2.get_position()+sam_record_r2.get_alignment_length()))
            
        current_open_file.write(sam_record_to_fastq(sam_record_r2,rgid=rgid))
        current_open_file.write('\n')
    if close_file_factory:
        file_factory.close()
    #genome_loader.close()
    return site_to_list_of_range


def process_bam_file_with_allele(known_sites_file, bam_file, restriction_site_seq='TGCAG', file_factory=None):
    known_sites = load_known_sites(known_sites_file)
    sam_pair_generator = load_from_sites_generator(known_sites, bam_file, restriction_site_seq)
    current_site=None
    current_open_file=None
    count_read=0
    all_sites_coverage={}
    close_file_factory=False
    if file_factory is None:
        file_factory=File_factory()
        close_file_factory=True
    site_to_list_of_range={}
    for sam_record_r1,sam_record_r2, site_info in sam_pair_generator:
        count_read+=1
        if count_read%10000==0:
            print count_read,file_factory.get_number_file_open()
        reference, position, strand, allele = site_info
        if current_site!=site_info:
            current_open_file = file_factory.get_openfile_from_site(reference, position, strand, allele)
            current_site=site_info
        
        if not all_sites_coverage.has_key(site_info):
            all_sites_coverage[site_info]=1
        else:
            all_sites_coverage[site_info]+=1
            
        rgid=sam_record_r1.get_tag('RG')
        #sam_record_r2 = fix_sam_record_from_reference(sam_record_r2, genome_loader)
        list_of_range = site_to_list_of_range.get(site_info)
        if not list_of_range:
            list_of_range=[]
            site_to_list_of_range[site_info]=list_of_range
        list_of_range.append((sam_record_r2.get_position(), sam_record_r2.get_position()+sam_record_r2.get_alignment_length()))
            
        current_open_file.write(sam_record_to_fastq(sam_record_r2,rgid=rgid))
        current_open_file.write('\n')
    if close_file_factory:
        file_factory.close()
    #genome_loader.close()
    return site_to_list_of_range

def get_range_per_site_from_bam_file(known_sites_file, bam_file, restriction_site_seq='TGCAG'):
    known_sites = load_known_sites(known_sites_file)
    sam_pair_generator = load_from_sites_generator(known_sites, bam_file, restriction_site_seq, options="-F 1024")
    count_read=0
    site_to_list_of_range={}
    for sam_record_r1,sam_record_r2, site_info in sam_pair_generator:
        count_read+=1
        tmp = site_to_list_of_range.get(site_info)
        if tmp is None:
            coverage=0
            list_of_range=[]
        else:
            coverage,list_of_range = tmp
        coverage+=1
#        list_of_range.append((sam_record_r1.get_position(), sam_record_r1.get_position()+sam_record_r1.get_alignment_length()))
        list_of_range.append((sam_record_r2.get_position(), sam_record_r2.get_position()+sam_record_r2.get_alignment_length()))
        site_to_list_of_range[site_info] = (coverage, list_of_range)
    return site_to_list_of_range

def get_range_per_site_from_multiple_bam_files(known_sites_file, bam_files, fragment_span_output, restriction_site_seq='TGCAG',with_allele=False):
    all_site_to_list_of_range={}
    for bam_file in bam_files:
        print "process %s "%bam_file
        site_to_list_of_range=get_range_per_site_from_bam_file(known_sites_file, bam_file, restriction_site_seq)
        all_site_to_list_of_range = merge_site_to_list_of_range(all_site_to_list_of_range,site_to_list_of_range)
    open_output=open(fragment_span_output,'w')
    for site_info in all_site_to_list_of_range.keys():
        coverage, tmp_ranges = all_site_to_list_of_range.get(site_info)
        tmp_ranges=merge_ranges(tmp_ranges, end_inclusive=True)
        new_ranges=[]
        for start,end in tmp_ranges:
            if end-start>101:
                new_ranges.append((start,end))
        if with_allele:
            site_str='site_%s_%s_%s_%s'%site_info
        else:
            site_str='site_%s_%s_%s'%site_info
        open_output.write('%s\t%s\t%s\n'%(site_str,coverage, '\t'.join(['%s-%s'%(start,end)for start,end in new_ranges])))
    open_output.close()
    

def process_multiple_bam_files(known_sites_file, bam_files, genome_file, snp_list_file=None, restriction_site_seq='TGCAG', with_allele=False):
    file_factory=File_factory()
    for bam_file in bam_files:
        print "process %s "%bam_file
        if with_allele:
            process_bam_file_with_allele(known_sites_file, bam_file, restriction_site_seq, file_factory)
        else:
            process_bam_file(known_sites_file, bam_file, genome_file, snp_list_file, restriction_site_seq, file_factory)
    file_factory.close()
    
    
        
def merge_site_to_list_of_range(site_to_list_of_range1, site_to_list_of_range2):
    site_to_list_of_range={}
    all_keys=set(site_to_list_of_range1.keys()).union(set(site_to_list_of_range2.keys()))
    for site_info in all_keys:
        ranges=[]
        coverage=0
        tmp1=site_to_list_of_range1.get(site_info)
        tmp2=site_to_list_of_range2.get(site_info)
        if tmp1:
            coverage1,ranges1=tmp1
            coverage+=coverage1
            ranges.extend(ranges1)
        if tmp2:
            coverage2,ranges2=tmp2
            coverage+=coverage2
            ranges.extend(ranges2)
        new_ranges = merge_ranges(ranges, end_inclusive=True)
        site_to_list_of_range[site_info]=(coverage,new_ranges)
    return site_to_list_of_range
        
def get_RAD_site_from_sam_record_old(sam_record, known_sites, restriction_site_seq):
    """From a list of known sites finds out if a site is associated with this read in sam format."""
    if sam_record.is_first_read() and not sam_record.is_unmapped():
        ref=sam_record.get_reference_name()
        pos=sam_record.get_position()
        align_length=sam_record.get_alignment_length()
        if sam_record.is_reverse_strand():
            site_position = pos+align_length-len(restriction_site_seq)
            strand='-'
        else:
            site_position = pos-1
            strand='+'
        if known_sites.has_key((ref, int(site_position))):
            return (ref, int(site_position), strand)
    return None

def get_RAD_site_from_sam_record(sam_record, known_sites, restriction_site_seq):
    """From a list of known sites finds out if a site is associated with this read in sam format.
    John uses a different coordinate for the restriction start site."""
    if sam_record.is_first_read() and not sam_record.is_unmapped():
        ref=sam_record.get_reference_name()
        pos=int(sam_record.get_alignment_start())
        align_length=sam_record.get_alignment_length()
        sequence = sam_record.get_query_sequence()
        if sam_record.is_reverse_strand():
            site_position = pos + align_length
            strand='-'
            sequence = DNA_tools.rev_complements(sequence)
        else:
            site_position = pos-1
            strand='+'

        if known_sites.has_key((ref, int(site_position), strand)):
            return (ref, int(site_position), strand)
        elif known_sites.has_key((ref, int(site_position), strand, sequence)):
            return (ref, int(site_position), strand, sequence)
    return None    
            
def sam_record_to_fastq(sam_record, rgid=None):
    out=[]
    if sam_record.is_first_read():
        suffix='/1'
    elif sam_record.is_second_read():
        suffix='/2'
    else:
        suffix=''
    if not sam_record.is_unmapped():
        pos_str='_%s_%s'%(sam_record.get_reference_name(),sam_record.get_position())
    else:
        pos_str=''
    if rgid:
        rgid_str='RGID:%s'%(rgid)
    else:
        rgid_str=''
    out.append('@%s%s%s%s'%(sam_record.get_query_name(), pos_str, rgid_str, suffix))
    if sam_record.is_reverse_strand():
        out.append(DNA_tools.rev_complements(sam_record.get_query_sequence()))
        out.append('+')
        out.append(sam_record.get_query_quality()[::-1])
    else:
        out.append(sam_record.get_query_sequence())
        out.append('+')
        out.append(sam_record.get_query_quality())
    return '\n'.join(out)

def sam_record_to_fastq_simple(sam_record, rgid=None):
    out=[]
    out.append('@%s'%(sam_record.get_query_name()))
    if sam_record.is_reverse_strand():
        out.append(DNA_tools.rev_complements(sam_record.get_query_sequence()))
        out.append('+')
        out.append(sam_record.get_query_quality()[::-1])
    else:
        out.append(sam_record.get_query_sequence())
        out.append('+')
        out.append(sam_record.get_query_quality())
    return '\n'.join(out)

def fix_sam_record_from_reference(sam_record, genome_loader):
    if not sam_record.is_unmapped():
        reference = sam_record.get_reference_name()
        position = sam_record.get_position()
        (header,sequence) = genome_loader.get_chr(reference)
        
        length = len(sam_record.get_query_sequence())
        sam_record.set_query_sequence(sequence[position-1:position+length-1])
        sam_record.set_cigar_string('%sM'%length)
    return sam_record



        
def main():
    #initialize the logging
    utils_logging.init_logging(logging.INFO)
    #Setup options
    optparser=_prepare_optparser()
    (options,args) = optparser.parse_args()
    #verify options
    arg_pass=_verifyOption(options)
    if not arg_pass:
        logging.warning(optparser.get_usage())
        logging.critical("Non valid arguments: exit")
        sys.exit(1)
    bam_files=[options.bam_file]
    if len(args)>0:
        bam_files.extend(args)
    if options.fragment_span_output:
        get_range_per_site_from_multiple_bam_files(known_sites_file=options.known_sites,
                                                   bam_files=bam_files,
                                                   fragment_span_output=options.fragment_span_output,
                                                   restriction_site_seq=options.restriction_site,
                                                   with_allele=options.with_allele)
    else:
        process_multiple_bam_files(known_sites_file=options.known_sites,
                               bam_files=bam_files,
                               genome_file=options.genome_file,
                               snp_list_file=options.snp_list_file,
                               restriction_site_seq=options.restriction_site,
                               with_allele=options.with_allele)

    


def _prepare_optparser():
    """Prepare optparser object. New options will be added in this
    function first.
    """
    usage = """usage: %prog <-b bam_file> <-g genome_file> <-k known_sites>"""
    description = """This script extract reads from an aligned bam file and create the corresponding fastq files."""
    
    optparser = OptionParser(description=description,usage=usage,add_help_option=False)
    optparser.add_option("-h","--help",action="help",help="show this help message and exit.")
    optparser.add_option("-b","--bam_file",dest="bam_file",type="string",
                         help="The bam file from which the reads should be extracted.")
    optparser.add_option("-g","--genome_file",dest="genome_file",type="string",
                         help="The genome file with the tag aligned to the genome.")
    optparser.add_option("-k","--known_sites",dest="known_sites",type="string",
                         help="The file containing the sites of interest that need to be extracted.")
    optparser.add_option("-r","--restriction_site",dest="restriction_site",type="string", default="TGCAG",
                         help="The sequenced part of the restriction site.")
    optparser.add_option("-s","--snp_list_file",dest="snp_list_file",type="string", help="list of SNPs to introduce in the reads")
    optparser.add_option("-f","--fragment_span_output",dest="fragment_span_output",type="string",
                         help="The file where the span for each site will be output.")
    optparser.add_option("-a","--with_allele",dest="with_allele",action="store_true",default=False,
                         help="create a site per allele as specified in the known sites file. Only perfect matches to the allele will be used")
    return optparser


def _verifyOption(options):
    """Check if the mandatory option are present in the options objects.
    @return False if any argument is wrong."""
    arg_pass=True
    
    if not options.bam_file:
        logging.error("You must specify a bam file -b.")
        arg_pass=False
    return arg_pass



if __name__=="__main__":
    main()        

if __name__=="__main__":
    bam_file=sys.argv[1]
    genome_file=sys.argv[2]
    know_sites_file=sys.argv[3]
    
    #bam_to_fastq(bam_file,genome_file)
